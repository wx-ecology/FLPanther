---
title: "Panther GLMM"
author: "Wenjing Xu"
date: "3/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
library(dplyr)
library(ggplot2)
library(GGally)
library(arm)
library(MCMCglmm)
library(plotMCMC)
library(reshape)
library(parallel)
library(aod)
library(fastDummies)

data <- read.csv("Final_Variable_all.csv")
veg.code <- read.csv("VegCombineCode.csv")
# reshape data
data <- data %>% dplyr::left_join(veg.code, by = "CLC_STATE")
data <- data %>% filter (!is.na(Fire_Cat))

```

## Data exploration
```{r data exploration}
# Ave_Dry and Ave_Wet correlated, select wet.
# Home range and DenID should only pick one.
data <- data %>% dplyr::select(Used, CatID, DenID, Fire_Cat, Age, Ave_Wet, Prec_Tree, dist2build, Combined) %>% dplyr:: rename(Veg_Class = Combined)
# specify factors
data$CatID <- as.factor(data$CatID)
data$DenID <- as.factor(data$DenID)
data$Fire_Cat <- as.factor(data$Fire_Cat)
data$Veg_Class <- factor(data$Veg_Class, levels = c("Upland Forest", "Other", "Human Modified", "Marsh-Shrub-Swamp", "Prairie-Grassland", "Wetland Forest"))

data.1 <- data %>% filter (Used == "1") %>% group_by(Fire_Cat) %>% summarise(used.count = n())
data.1
```
We have a lot of points in U. Unbalanced dataset.

```{r colinearity}
cor(data[,c(5:8)]) #does not seem to have strong linear relations
ggpairs(data[, c("Age", "Ave_Wet", "Prec_Tree", "dist2build")])
```

This plot further confirmed that there are no obvious linear relations. Yet, it seems like dist2build has very skewed distribution (lots of 0s). So later when scaling, we will conduct a log(x+1) transformation for dist2biuld before standardizing.

```{r categorical variables}
# all points
par(mfrow = c(1, 2))
ggplot(data, aes(x = Fire_Cat, y = CatID)) +
  stat_sum(aes(size = ..n.., group = Fire_Cat)) +
  scale_size_area(max_size=10)
# only den 
# ggplot(data %>% filter (Used == "1"), aes(x = Fire_Cat, y = CatID)) +
#   stat_sum(aes(size = ..n.., group = Fire_Cat)) +
#   scale_size_area(max_size=10)
```
Lots of U.

```{r}
data.2 <- melt(data[, c("Used", "Age", "Ave_Wet", "Prec_Tree", "dist2build")], id.vars="Used")
ggplot(data.2, aes(factor(Used), y = value, fill=factor(Used))) +
    geom_boxplot() +
    facet_wrap(~variable, scales="free_y")

```
Super clear that they pick high tree precentage area. slightly far away from building, almost no difference in age and ave wet. We will see in the real model. 


```{r scaling}
# scaling numeric variables by deviding two standard deviations for binary predictors
# http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf
data.scaled <- data
data.scaled$dist2build <- log(data.scaled$dist2build + 1)
data.scaled[,5:8] <- sapply(data.scaled[,5:8], rescale, binary.inputs = "full")
#data.scaled$Ave_Wet <- sqrt(data.scaled$Ave_Wet)
#data.scaled$Prec_Tree <- sqrt(data.scaled$Prec_Tree)  # issue: take sqrt creates lots of NA 
ggpairs(data.scaled[, c("Age", "Ave_Wet", "Prec_Tree", "dist2build")])

```
The table shows we have a unbalanced dataset in terms of Fire_Cat. In principle, lme4 can deal with unbalanced data sets but the low number of data points in some cells of the design means that it is hard to estimate some of the effects. 

Dist2building looks pretty skewed. 

## fitting and diagnostic glmer
```{r fitting glmer}
# full model
m.0 <- glmer(Used ~ Fire_Cat + Prec_Tree + Veg_Class + Age + Ave_Wet + dist2build + (1|CatID), data = data.scaled, family = binomial, nAGQ = 10)
#print(m.0, correlation = FALSE)
summary(m.0)
# the next model treat catID as fixed variable. But should not do that becasue too many levels. 
# m.1 <- glm(Used ~ Fire_Cat + Prec_Tree + Veg_Class + Prec_Tree + Age + Ave_Wet + dist2build + CatID, data = data.scaled, family = binomial)

m.2 <- glm(Used ~ Fire_Cat + Prec_Tree + Veg_Class + Age + Ave_Wet + dist2build, data = data.scaled, family = binomial)
#print(m.2, correlation = FALSE)
summary(m.2)

# with one less variable
m.3 <- glm(Used ~ Fire_Cat + Prec_Tree + Veg_Class + Ave_Wet + dist2build, data = data.scaled, family = binomial)
m.4 <- glm(Used ~ Fire_Cat + Prec_Tree + Veg_Class + Age + dist2build, data = data.scaled, family = binomial)
m.5 <- glm(Used ~ Fire_Cat + Prec_Tree + Veg_Class + Age + Ave_Wet, data = data.scaled, family = binomial)

# with two less variables
m.6 <- glm(Used ~ Fire_Cat + Prec_Tree + Veg_Class + dist2build, data = data.scaled, family = binomial)
m.7 <- glm(Used ~ Fire_Cat + Prec_Tree + Veg_Class + Age, data = data.scaled, family = binomial)
m.8 <- glm(Used ~ Fire_Cat + Prec_Tree + Veg_Class + Ave_Wet, data = data.scaled, family = binomial)

m.9 <- glm(Used ~ Fire_Cat + Prec_Tree + Veg_Class , data = data.scaled, family = binomial)
summary(m.9)
```
m.0 and m.2 are besically identical because the random factor could not be taken into consideration in m.0

The below is a test to see whether the model is significantly better than null model - and they all seems better than null model. 
``` {r evaluation}
foo <- function(x) with(x, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
foo(m.2)
```

Compare models
``` {r compare models}
#first, analysis of deviance
#The deviance is somewhat analogous to the variance analyzed in an ANOVA, at least to the extent that the goal of modeling is to explain as much as possible of deviance. 
anova(m.2, m.3,m.4, m.5, test = "Chi")
AIC(m.2, m.3,m.4, m.5, m.6, m.7, m.8, m.9)
```
All super similar AIC... 

```{r confidence interval} 
# this confident interval is calculated based on maximum likihood
# note that the numbers here are log-transformed
co.table <- cbind(OR = coef(m.2), confint(m.2))
co.df <- data.frame(variable = row.names(co.table), estimate = co.table[1:nrow(co.table), 1], LL = co.table[1:nrow(co.table), 2], UL =  co.table[1:nrow(co.table), 3])
co.df$variable <- as.character(co.df$variable)

ggplot(co.df, aes(y=estimate, x=variable, ymin=LL, ymax=UL)) + geom_pointrange() + coord_flip()
```
For every unit increase in tree percent, the *log odds* of den presence increase for 1.22. For fire, change from category A to B, the *log odds* for den presence increase for 5.1


We can test for an overall effect of rank using the wald.test function of the aod library. The order in which the coefficients are given in the table of coefficients is the same as the order of the terms in the model. This is important because the wald.test function refers to the coefficients by their order in the model. We use the wald.test function.

``` {r overall effect}
wald.test(b = coef(m.2), Sigma = vcov(m.2), Terms = 1:5)
```
The chi-squared test statistic of 308.8, with 5 degrees of freedom is associated with a p-value of 0.00 indicating that the overall effect of rank is statistically significant.


We can also test additional hypotheses about the differences in the coefficients for the different levels of rank. To contrast these two terms, we multiply one of them by 1, and the other by -1. The other terms in the model are not involved in the test, so they are multiplied by 0. The second line of code below uses L=l to tell R that we wish to base the test on the vector l (rather than using the Terms option as we did above).
``` {r additional hypotheses}
l <- cbind(1, -1, 0, 0, 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0)
wald.test(b = coef(m.2), Sigma = vcov(m.2), L = l)

l <- cbind(0, 0, 0, 1, -1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0,0 )
wald.test(b = coef(m.2), Sigma = vcov(m.2), L = l)
```
It looks like only A is significantly different from other fire cat. 

Now exponentiate the coefficients and interpret them as odds-ratios. 
```{r}
### odds ratios and 95% CI
exp(cbind(OR = coef(m.2), confint(m.2)))
```
For every unit increase in tree percent, the odds of den presence increase for 6.756 (note that one unit is like from 0 precent to 100% tree. but in reality it is limited from 0 to 1). For fire, change from category A to B, the odds for den presence increase for 1.131. Or, when holding all other variables as fixed, Fire Cat B is 13% more likely to have dens then newly burned area. 



Create a table to plot predict probabilities varying percent tree
``` {r predict - tree prec by fire cat}
newdata <- with(data.scaled[4:9], data.frame(Fire_Cat = factor(rep(c("A", "B", "C", "D", "U"), each = 100)),
                                                          Age = mean(data.scaled$Age),
                                                          Ave_Wet = mean(data.scaled$Ave_Wet), 
                                                          Prec_Tree = rep(seq(from = -1, to = 1, length.out = 100), 5), 
                                                          dist2build = mean(data.scaled$dist2build),
                                                          Veg_Class = "Other"))

newdata <- cbind(newdata, predict(m.2, newdata = newdata, type = "link", se = TRUE))
newdata <- within(newdata, {
  PredictedProb <- plogis(fit)
  LL <- plogis(fit - (1.96 * se.fit))
  UL <- plogis(fit + (1.96 * se.fit))
})

p.1 <- ggplot(newdata, aes(x = Prec_Tree, y = PredictedProb)) + geom_ribbon(aes(ymin = LL,
    ymax = UL, fill = Fire_Cat), alpha = 0.2) + geom_line(aes(colour = Fire_Cat),
    size = 1)
p.1
```

``` {r fire cat}
newdata2 <- with(data.scaled[4:9], data.frame(Fire_Cat = factor(rep(c("A", "B", "C", "D", "U"), each = 5)),
                                                          Age = mean(data.scaled$Age),
                                                          Ave_Wet = mean(data.scaled$Ave_Wet), 
                                                          Prec_Tree = mean(data.scaled$Prec_Tree), 
                                                          dist2build = mean(data.scaled$dist2build),
                                                          Veg_Class = rep(c("Other", "Marsh-Shrub-Swamp", "Prairie-Grassland", "Upland Forest", "Wetland Forest"), 5)))

newdata2 <- cbind(newdata2, predict(m.2, newdata = newdata2, type = "link", se = TRUE))
newdata2 <- within(newdata2, {
  PredictedProb <- plogis(fit)
  LL <- plogis(fit - (1.96 * se.fit))
  UL <- plogis(fit + (1.96 * se.fit))
})

p.2 <- ggplot(newdata2, aes(x = Fire_Cat, y = PredictedProb, colour = Veg_Class)) + 
  geom_point() + 
  geom_linerange(aes(ymin = LL, ymax = UL))
p.2
```
"Other" is significantly higher than other classes. Need to further tease apart different classes.




















<!-- ########################################################################### -->
<!-- ########################################################################### -->
<!-- ########################################################################### -->
<!-- ``` {r bootstrapping} -->
<!-- sampler <- function(dat, clustervar, replace = TRUE, reps = 1) { -->
<!--     cid <- unique(dat[, clustervar[1]]) -->
<!--     ncid <- length(cid) -->
<!--     recid <- sample(cid, size = ncid * reps, replace = TRUE) -->
<!--     if (replace) { -->
<!--         rid <- lapply(seq_along(recid), function(i) { -->
<!--             cbind(NewID = i, RowID = sample(which(dat[, clustervar] == recid[i]), -->
<!--                 size = length(which(dat[, clustervar] == recid[i])), replace = TRUE)) -->
<!--         }) -->
<!--     } else { -->
<!--         rid <- lapply(seq_along(recid), function(i) { -->
<!--             cbind(NewID = i, RowID = which(dat[, clustervar] == recid[i])) -->
<!--         }) -->
<!--     } -->
<!--     dat <- as.data.frame(do.call(rbind, rid)) -->
<!--     dat$Replicate <- factor(cut(dat$NewID, breaks = c(1, ncid * 1:reps), include.lowest = TRUE, -->
<!--         labels = FALSE)) -->
<!--     dat$NewID <- factor(dat$NewID) -->
<!--     return(dat) -->
<!-- } -->

<!-- set.seed(20) -->
<!-- tmp <- sampler(data.scaled, "DenID", reps = 100) -->
<!-- bigdata <- cbind(tmp, data.scaled[tmp$RowID, ]) -->

<!-- f <- fixef(m.0) -->
<!-- r <- getME(m.0, "theta") -->

<!-- cl <- makeCluster(4) -->
<!-- clusterExport(cl, c("bigdata", "f", "r")) -->
<!-- clusterEvalQ(cl, require(lme4)) -->

<!-- myboot <- function(i) { -->
<!--     object <- try(glmer(Used ~ Fire_Cat + Prec_Tree + Veg_Class + Prec_Tree + Age + Ave_Wet + dist2build + (1|DenID), data = bigdata, subset = Replicate == i, family = binomial, -->
<!--         nAGQ = 1, start = list(fixef = f, theta = r)), silent = TRUE) -->
<!--     if (class(object) == "try-error") -->
<!--         return(object) -->
<!--     c(fixef(object), getME(object, "theta")) -->
<!-- } -->

<!-- start <- proc.time() -->
<!-- res <- parLapplyLB(cl, X = levels(bigdata$Replicate), fun = myboot) -->
<!-- end <- proc.time() -->

<!-- # shut down the cluster -->
<!-- stopCluster(cl) -->

<!-- # calculate proportion of models that successfully converged -->
<!-- success <- sapply(res, is.numeric) -->
<!-- mean(success) -->

<!-- # combine successful results -->
<!-- bigres <- do.call(cbind, res[success]) -->

<!-- # calculate 2.5th and 97.5th percentiles for 95% CI -->
<!-- (ci <- t(apply(bigres, 1, quantile, probs = c(0.025, 0.975)))) -->

<!-- # All results -->
<!-- finaltable <- cbind(Est = c(f, r), SE = c(se, NA), BootMean = rowMeans(bigres), -->
<!--     ci) -->
<!-- # round and print -->
<!-- round(finaltable, 3) -->
<!-- ``` -->


<!-- ## Mar 3 2020: keep going  -->
<!-- https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/  -->

<!-- ``` {r} -->
<!-- p1 <- plot(m.1,id=0.05) -->
<!-- plot(p1) -->
<!-- ``` -->
<!-- glmm has singularity issue. "One alternative (suggested by Robert LaBudde) for the small-numbers-of-levels scenario is to “fit the model with the random factor as a fixed effect, get the level coefficients in the sum to zero form, and then compute the standard deviation of the coefficients.” This is appropriate for users who are (a) primarily interested in measuring variation (i.e. the random effects are not just nuisance parameters, and the variability [rather than the estimated values for each level] is of scientific interest), (b) unable or unwilling to use other approaches (e.g. MCMC with half-Cauchy priors in WinBUGS), (c) unable or unwilling to collect more data. For the simplest case (balanced, orthogonal, nested designs with normal errors) these estimates of standard deviations should equal the classical method-of-moments estimates." -->


<!-- ```{r} -->

<!-- ``` -->
